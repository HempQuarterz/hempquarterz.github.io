#!/usr/bin/env node

/**
 * Dead Sea Scrolls Data Cleaning Script v2
 *
 * Enhanced version that handles:
 * - Cross-file duplicates (book/chapter/verse)
 * - Within-batch duplicates (prevents import conflicts)
 * - Invalid verse/chapter numbers
 * - Batch-aware sorting to prevent conflicts
 *
 * Key Improvement: Groups duplicates and keeps only ONE canonical version
 * per book/chapter/verse, ensuring no duplicates appear in ANY 100-line batch
 *
 * Input:  manuscripts/dead-sea-scrolls/dss-full.json (52,769 lines)
 * Output: manuscripts/dead-sea-scrolls/dss-cleaned-v2.json (~52,080 lines)
 *         database/dss-cleanup-report-v2.json (detailed report)
 *
 * Usage:
 *   node database/clean-dss-data-v2.js
 */

const fs = require('fs');
const path = require('path');

console.log('\n╔════════════════════════════════════════════════════════════════╗');
console.log('║   Dead Sea Scrolls Data Cleaning Script v2 (Batch-Aware)    ║');
console.log('╚════════════════════════════════════════════════════════════════╝\n');

// File paths
const INPUT_FILE = path.join(__dirname, '../manuscripts/dead-sea-scrolls/dss-full.json');
const OUTPUT_FILE = path.join(__dirname, '../manuscripts/dead-sea-scrolls/dss-cleaned-v2.json');
const REPORT_FILE = path.join(__dirname, 'dss-cleanup-report-v2.json');

const BATCH_SIZE = 100; // Import batch size

// Tracking objects
const cleanupStats = {
  totalLines: 0,
  duplicatesRemoved: 0,
  invalidVersesFixed: 0,
  invalidChaptersFixed: 0,
  cleanLines: 0,
  duplicateGroups: 0,
  duplicateExamples: [],
  invalidVerseExamples: [],
  invalidChapterExamples: []
};

/**
 * Load and parse DSS data
 */
function loadData() {
  console.log('📖 Loading DSS data...');

  if (!fs.existsSync(INPUT_FILE)) {
    console.error(`❌ Input file not found: ${INPUT_FILE}`);
    process.exit(1);
  }

  const rawData = fs.readFileSync(INPUT_FILE, 'utf8');
  const data = JSON.parse(rawData);

  cleanupStats.totalLines = data.verses.length;
  console.log(`✅ Loaded ${cleanupStats.totalLines} lines\n`);
  return data;
}

/**
 * Create unique key for verse
 */
function makeKey(verse) {
  return `${verse.book}-${verse.chapter}-${verse.verse}`;
}

/**
 * Fix invalid chapter/verse numbers
 */
function fixInvalidNumbers(verse, originalIndex) {
  const cleaned = { ...verse };
  let changed = false;

  // Fix invalid chapter (chapter <= 0)
  if (cleaned.chapter <= 0) {
    cleanupStats.invalidChaptersFixed++;

    if (cleanupStats.invalidChapterExamples.length < 10) {
      cleanupStats.invalidChapterExamples.push({
        book: cleaned.book,
        originalChapter: cleaned.chapter,
        originalVerse: cleaned.verse,
        fixedChapter: 1,
        text: cleaned.text.substring(0, 50)
      });
    }

    cleaned.chapter = 1;
    changed = true;
  }

  // Fix invalid verse (verse <= 0)
  if (cleaned.verse <= 0) {
    cleanupStats.invalidVersesFixed++;

    if (cleanupStats.invalidVerseExamples.length < 10) {
      cleanupStats.invalidVerseExamples.push({
        book: cleaned.book,
        chapter: cleaned.chapter,
        originalVerse: cleaned.verse,
        fixedVerse: 1,
        text: cleaned.text.substring(0, 50)
      });
    }

    cleaned.verse = 1;
    changed = true;
  }

  return cleaned;
}

/**
 * Group verses by book/chapter/verse key
 * For duplicates, keep only the FIRST occurrence
 */
function groupAndDeduplicateVerses(verses) {
  console.log('🔧 Grouping and deduplicating...\n');

  const verseMap = new Map(); // key -> verse object
  const duplicateGroups = new Map(); // key -> array of indices

  verses.forEach((verse, index) => {
    // First fix invalid numbers
    const fixed = fixInvalidNumbers(verse, index);
    const key = makeKey(fixed);

    if (!verseMap.has(key)) {
      // First occurrence - keep it
      verseMap.set(key, {
        verse: fixed,
        originalIndex: index
      });
    } else {
      // Duplicate - track it
      if (!duplicateGroups.has(key)) {
        duplicateGroups.set(key, [verseMap.get(key).originalIndex]);
      }
      duplicateGroups.get(key).push(index);
      cleanupStats.duplicatesRemoved++;
    }

    // Progress indicator
    if ((index + 1) % 5000 === 0) {
      process.stdout.write(`   Progress: ${index + 1}/${verses.length} (${Math.round((index + 1) / verses.length * 100)}%)\r`);
    }
  });

  console.log(`   Progress: ${verses.length}/${verses.length} (100%)\n`);

  // Save duplicate examples
  cleanupStats.duplicateGroups = duplicateGroups.size;
  let exampleCount = 0;
  for (const [key, indices] of duplicateGroups.entries()) {
    if (exampleCount >= 10) break;
    cleanupStats.duplicateExamples.push({
      key,
      occurrences: indices.length,
      indices: indices.slice(0, 5) // Show first 5 indices
    });
    exampleCount++;
  }

  // Extract unique verses
  const uniqueVerses = Array.from(verseMap.values())
    .map(entry => entry.verse);

  cleanupStats.cleanLines = uniqueVerses.length;

  console.log(`📊 Deduplication Results:`);
  console.log(`   • Input lines: ${verses.length}`);
  console.log(`   • Unique verses: ${uniqueVerses.length}`);
  console.log(`   • Duplicates removed: ${cleanupStats.duplicatesRemoved}`);
  console.log(`   • Duplicate groups: ${cleanupStats.duplicateGroups}`);
  console.log(`   • Invalid chapters fixed: ${cleanupStats.invalidChaptersFixed}`);
  console.log(`   • Invalid verses fixed: ${cleanupStats.invalidVersesFixed}\n`);

  return uniqueVerses;
}

/**
 * Verify no duplicates exist in any batch
 */
function verifyBatchUniqueness(verses) {
  console.log('🔍 Verifying batch uniqueness...\n');

  let conflictBatches = 0;
  const conflictExamples = [];

  for (let batchStart = 0; batchStart < verses.length; batchStart += BATCH_SIZE) {
    const batchEnd = Math.min(batchStart + BATCH_SIZE, verses.length);
    const batch = verses.slice(batchStart, batchEnd);

    // Check for duplicates within this batch
    const batchKeys = new Set();
    const batchDuplicates = [];

    batch.forEach((verse, index) => {
      const key = makeKey(verse);
      if (batchKeys.has(key)) {
        batchDuplicates.push({ key, position: batchStart + index });
      }
      batchKeys.add(key);
    });

    if (batchDuplicates.length > 0) {
      conflictBatches++;
      if (conflictExamples.length < 5) {
        conflictExamples.push({
          batchRange: `${batchStart}-${batchEnd}`,
          duplicates: batchDuplicates
        });
      }
    }
  }

  if (conflictBatches > 0) {
    console.log(`❌ Found ${conflictBatches} batches with internal duplicates!`);
    console.log(`   This should NOT happen - deduplication failed!\n`);
    console.log('Examples:');
    conflictExamples.forEach(ex => {
      console.log(`   Batch ${ex.batchRange}: ${ex.duplicates.length} duplicates`);
      ex.duplicates.slice(0, 3).forEach(dup => {
        console.log(`      - ${dup.key} at position ${dup.position}`);
      });
    });
    console.log('');
    return false;
  }

  console.log(`✅ All ${Math.ceil(verses.length / BATCH_SIZE)} batches are duplicate-free!\n`);
  return true;
}

/**
 * Save cleaned data
 */
function saveCleanedData(data, verses) {
  console.log('💾 Saving cleaned data...\n');

  const output = {
    ...data,
    verses
  };

  fs.writeFileSync(OUTPUT_FILE, JSON.stringify(output, null, 2));

  const stats = fs.statSync(OUTPUT_FILE);
  const sizeMB = (stats.size / 1024 / 1024).toFixed(2);

  console.log(`✅ Cleaned data saved: ${OUTPUT_FILE}`);
  console.log(`   Size: ${sizeMB} MB\n`);
}

/**
 * Generate cleanup report
 */
function saveReport() {
  console.log('📊 Generating cleanup report...\n');

  const report = {
    timestamp: new Date().toISOString(),
    input_file: INPUT_FILE,
    output_file: OUTPUT_FILE,
    batch_size: BATCH_SIZE,
    statistics: {
      total_lines_input: cleanupStats.totalLines,
      total_lines_output: cleanupStats.cleanLines,
      duplicates_removed: cleanupStats.duplicatesRemoved,
      duplicate_groups: cleanupStats.duplicateGroups,
      invalid_verses_fixed: cleanupStats.invalidVersesFixed,
      invalid_chapters_fixed: cleanupStats.invalidChaptersFixed,
      total_changes: cleanupStats.duplicatesRemoved + cleanupStats.invalidVersesFixed + cleanupStats.invalidChaptersFixed
    },
    examples: {
      duplicates: cleanupStats.duplicateExamples,
      invalid_verses: cleanupStats.invalidVerseExamples,
      invalid_chapters: cleanupStats.invalidChapterExamples
    }
  };

  fs.writeFileSync(REPORT_FILE, JSON.stringify(report, null, 2));
  console.log(`✅ Cleanup report saved: ${REPORT_FILE}\n`);
}

/**
 * Display summary
 */
function displaySummary() {
  console.log('╔════════════════════════════════════════════════════════════════╗');
  console.log('║                    CLEANUP SUMMARY                            ║');
  console.log('╚════════════════════════════════════════════════════════════════╝\n');

  console.log(`📥 Input:  ${cleanupStats.totalLines.toLocaleString()} lines`);
  console.log(`📤 Output: ${cleanupStats.cleanLines.toLocaleString()} lines\n`);

  console.log('🔧 Changes Made:\n');
  console.log(`   • Duplicates removed:      ${cleanupStats.duplicatesRemoved}`);
  console.log(`   • Duplicate groups:        ${cleanupStats.duplicateGroups}`);
  console.log(`   • Invalid verses fixed:    ${cleanupStats.invalidVersesFixed} (verse 0 → 1)`);
  console.log(`   • Invalid chapters fixed:  ${cleanupStats.invalidChaptersFixed} (chapter 0 → 1)`);
  console.log(`   • Total changes:           ${cleanupStats.duplicatesRemoved + cleanupStats.invalidVersesFixed + cleanupStats.invalidChaptersFixed}\n`);

  console.log('📊 Expected Import Results:\n');
  console.log(`   • Lines to import:         ${cleanupStats.cleanLines.toLocaleString()}`);
  console.log(`   • Expected success rate:   100% (all duplicates resolved)`);
  console.log(`   • No batch conflicts:      Guaranteed\n`);

  console.log('✅ Data cleaning complete!\n');

  console.log('══════════════════════════════════════════════════════════════════════\n');
  console.log('📋 NEXT STEPS:\n');
  console.log('1. Clear partial DSS import from database:');
  console.log('   DELETE FROM verses WHERE manuscript_id = \'<DSS_ID>\';\n');
  console.log('2. Update import script to use cleaned data:');
  console.log('   cd hempquarterz.github.io');
  console.log('   # Modify import-dead-sea-scrolls.js to load dss-cleaned-v2.json');
  console.log('   node database/import-dead-sea-scrolls.js --full\n');
  console.log('3. Expected result:');
  console.log(`   • All ${cleanupStats.cleanLines.toLocaleString()} lines imported successfully`);
  console.log('   • 997 unique scrolls (all scrolls included)');
  console.log('   • 100% success rate (no conflicts)\n');
  console.log('══════════════════════════════════════════════════════════════════════\n');
  console.log('🎯 Mission: "Restoring truth, one name at a time."');
  console.log('✨ DSS data ready for 100% successful re-import!\n');
}

/**
 * Main execution
 */
async function main() {
  const startTime = Date.now();

  try {
    // Load data
    const data = loadData();

    // Clean and deduplicate
    const cleanedVerses = groupAndDeduplicateVerses(data.verses);

    // Verify batch uniqueness
    const isValid = verifyBatchUniqueness(cleanedVerses);

    if (!isValid) {
      console.error('❌ Batch validation failed! Cannot proceed with import.');
      process.exit(1);
    }

    // Save cleaned data
    saveCleanedData(data, cleanedVerses);

    // Generate report
    saveReport();

    // Display summary
    displaySummary();

    const duration = ((Date.now() - startTime) / 1000).toFixed(2);
    console.log(`⏱️  Total time: ${duration}s\n`);

  } catch (error) {
    console.error(`\n❌ Error: ${error.message}`);
    console.error(error.stack);
    process.exit(1);
  }
}

// Run
main();
